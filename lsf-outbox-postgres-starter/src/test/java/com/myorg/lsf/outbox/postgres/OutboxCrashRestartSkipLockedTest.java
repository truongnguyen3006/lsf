package com.myorg.lsf.outbox.postgres;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.myorg.lsf.contracts.core.envelope.EventEnvelope;
import com.myorg.lsf.outbox.OutboxWriter;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.boot.test.context.TestConfiguration;
import org.springframework.context.annotation.Bean;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.kafka.support.serializer.JsonSerializer;
import org.springframework.kafka.test.EmbeddedKafkaBroker;
import org.springframework.kafka.test.context.EmbeddedKafka;
import org.springframework.kafka.test.utils.KafkaTestUtils;

import java.time.Duration;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.atomic.AtomicBoolean;

import static org.junit.jupiter.api.Assertions.*;

@EmbeddedKafka(partitions = 1, topics = {OutboxCrashRestartTest.TOPIC})
@SpringBootTest(
        classes = {OutboxPublisherITApp.class, OutboxCrashRestartSkipLockedTest.HooksConfig.class},
        properties = {
                "lsf.outbox.enabled=true",
                "lsf.outbox.publisher.enabled=true",
                "lsf.outbox.publisher.scheduling-enabled=false",
                "lsf.outbox.publisher.batch-size=10",
                "lsf.outbox.publisher.lease=200ms",
                "lsf.outbox.publisher.backoff-base=50ms",
                "lsf.outbox.publisher.backoff-max=500ms",
                "lsf.outbox.publisher.max-retries=5",
                "lsf.outbox.publisher.send-timeout=5s",


                "spring.kafka.bootstrap-servers=${spring.embedded.kafka.brokers}",
                "spring.kafka.consumer.auto-offset-reset=earliest",
                "lsf.outbox.publisher.claim-strategy=SKIP_LOCKED"
        }
)
class OutboxCrashRestartSkipLockedTest extends PostgresContainerBase  {

    static final String TOPIC = "demo-topic";

    @Autowired OutboxWriter writer;
    @Autowired OutboxPublisher publisher;
    @Autowired JdbcOutboxRepository repo;

    @Autowired ConsumerFactory<String, EventEnvelope> consumerFactory;
    @Autowired ObjectMapper mapper;

    @Test
    void crashAfterClaim_thenRestart_shouldStillPublish() throws Exception {
        String eventId = "E_CRASH_" + java.util.UUID.randomUUID();

        EventEnvelope env = EventEnvelope.builder()
                .eventId(eventId)
                .eventType("demo.order.created.v1")
                .version(1)
                .producer("it")
                .occurredAtMs(System.currentTimeMillis())
                .payload(mapper.createObjectNode().put("x", 1))
                .build();

        writer.append(env, TOPIC, "k1");

        // 1st run: crash after claim (hook throws)
        try {
            publisher.runOnce();
            fail("Expected simulated crash");
        } catch (RuntimeException ignored) {}

        assertEquals("PROCESSING", repo.statusByEventId(eventId));

        // wait lease expire
        Thread.sleep(250);

        // 2nd run: should reclaim + publish + mark SENT
        publisher.runOnce();

        // verify Kafka got the event
        Consumer<String, EventEnvelope> c = consumerFactory.createConsumer("it", "it");
        c.subscribe(java.util.List.of(TOPIC));
        ConsumerRecord<String, EventEnvelope> rec =
                KafkaTestUtils.getSingleRecord(c, TOPIC, Duration.ofSeconds(5));
        assertNotNull(rec.value());
        assertEquals(eventId, rec.value().getEventId());

        // verify DB status
        assertEquals("SENT", repo.statusByEventId(eventId));
        c.close();
    }

    @TestConfiguration
    static class HooksConfig {
        private final AtomicBoolean crashOnce = new AtomicBoolean(true);

        @Bean
        OutboxPublisherHooks outboxPublisherHooks() {
            return new OutboxPublisherHooks() {
                @Override
                public void afterClaim(java.util.List<OutboxRow> claimedRows) {
                    if (crashOnce.compareAndSet(true, false)) {
                        throw new RuntimeException("Simulated crash after claim");
                    }
                }
            };
        }

        @Bean
        ConsumerFactory<String, EventEnvelope> consumerFactory(
                @Value("${spring.embedded.kafka.brokers}") String brokers
        ) {
            Map<String, Object> props = new HashMap<>();
            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
            props.put(ConsumerConfig.GROUP_ID_CONFIG, "it-g1");
            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
            props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);

            props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
            props.put(JsonDeserializer.VALUE_DEFAULT_TYPE,
                    "com.myorg.lsf.contracts.core.envelope.EventEnvelope");

            return new DefaultKafkaConsumerFactory<>(props);
        }

        @Bean
        KafkaTemplate<String, Object> kafkaTemplate(@Value("${spring.embedded.kafka.brokers}") String brokers) {
            Map<String, Object> props = new java.util.HashMap<>();
            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
            return new KafkaTemplate<>(new DefaultKafkaProducerFactory<>(props));
        }

    }

    @Autowired org.springframework.jdbc.core.JdbcTemplate jdbc;

    @org.junit.jupiter.api.BeforeEach
    void clean() {
        jdbc.update("TRUNCATE TABLE lsf_outbox");
    }

}
